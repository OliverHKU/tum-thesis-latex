% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Data Analysis}\label{chapter:data_analysis}
This chapter discusses how data analysis was done. The objective of the data analysis is to build a stress-eating profile for each candidate selected with the data recorded from the conversation between this candidate and the chatbot. Specifically, the profile was generated by using some prediction models which given the stress levels of the candidate at a random moment or a random day, predicts whether the candidate is likely to eat more or less than usual, how many pieces (as defined in previous chapters) he or she is likely to eat, and whether he or she tends to eat comfort food. Since all these predicted (output) features are discrete, a classification model was to be built for each output feature. Hence, the problem is formalized into the following steps:\bigskip

\noindent For each candidate,
\begin{enumerate}
  \item Extract the stress levels, \emph{more\_or\_less} entries, number of pieces, and \emph{comfort\_food} entries from the raw data
  \item Pre-process the data by engineering additional input features from the stress levels
  \item For each output feature, build a number of classification models using different underlying models
  \item For each output feature, evaluate and select the model to be used for prediction
  \item Given the sample inputs that represent certain stress conditions, predict the output features
  \item Use the predictions to generate a text-based stress-eating report
\end{enumerate}

Therefore, this chapter will first show the data gathered and the participants selected for further user studies, followed by a demonstration of data pre-processing as preparation for model training. As there are a number of options available for training classification models, it then presents how specific models were selected. Finally, it shows the building of stress-eating profiles according to the predictions made by selected models.

\section{Data Gathered}
Among the 33 participants, 5 were selected as potential candidates for further user studies, as the other 28 users had reported fewer than 20 \emph{tell\_stress\_level} entries, which are too few for data analysis. Among the 5 potential candidates, one had reported fewer than 10 food entries, thus eliminated from the user studies. Among the selected candidates, each of them had reported between 21 and 75 stress entries.

\section{Data Pre-Processing}
For each of the four selected candidates, an entry-level and a day-level dataset was built. Each data point in the entry-level represents one timestamp at which the candidate reported stress. On the contrary, each data point in the day-level represents a single day in which the candidate reported stress at least for once. This setting was chosen because, on the one hand, entry-level input is generally more frequent (most candidates reported stress levels more than once per day), but on the other, the "more or less" and "number of pieces" data was collected on a daily basis, hence the day-level.

For the entry-level data, in addition to its single stress level, there were three further features engineered to prevent underfitting (\cite{42_underfitting}), namely average stress level during the day, highest stress level during the day, and the standard deviation of the stress levels during the day. Since food description is the only piece of information related to entry-level stress data, the boolean value \emph{comfort\_food} becomes the only target to be predicted.

For the day-level data, the three statistics mentioned above (average stress level during the day, highest stress level during the day, and the standard deviation of the stress levels during the day) were selected as input features. The output features were \emph{comfort\_food}, \emph{more\_or\_less} and \emph{pieces}. The problem was then transformed into three sub-problems, within which \emph{comfort\_food}, \emph{more\_or\_less} and \emph{pieces} were predicted with 3 independent models.

Among the four candidates, only one had more than 10 entry-level food descriptions to construct enough data points for comfort food, therefore entry-level models were built only for this candidate, while day-level models were built for all. Additionally, one of the four candidates did not provide more than 10 \emph{more\_or\_less} and \emph{comfort\_food} data entries, hence eliminated from building these particular models. \autoref{tab:models} summarizes all training tasks for the candidates.

\begin{table}[htpb]
  \caption[Models Trained]{Summarization of training tasks for the participants}\label{tab:models}
  \centering
  \tiny
  \begin{tabular}{l l l l l}
    \toprule
      ParticipantID & Day-Level Comfort Food & Day-Level More or Less & Day-Level Pieces & Entry-Level Comfort Food \\
    \midrule
      1 & x & x & x & x \\
      2 & x & x & x & - \\
      3 & x & x & x & - \\
      4 & - & - & x & - \\
    \bottomrule
  \end{tabular}
\end{table}

\bigskip
\section{Model Selection}
Given the limited amount of data, neural network approaches were not considered. Alternatively, multiple traditional machine learning methods were tested, including decision tree, support vector machine, k-nearest neighbors, and Naive Bayes. These are the most popular methods for multiclass classification (\cite{43_multiclass_classification}). For each model, 5-fold cross-validation was used to test the performance. Since the objective was to build user-specific profiles, the model used for each user per task was individual. Therefore, the best performing model was selected for each training task, and these models do not have to be trained using the same method across tasks. The performance was measured by the accuracy of the cross-validation. Scikit-Learn (\cite{44_sklearn}) offers handy tools to fulfill the model selection tasks. After the model selection, the best performing models were used to fit all the data available in each task and train the prediction models.

\section{Profile Generation}
In order to predict the candidates' eating behaviors under stress, four sample inputs were designed for the day-level to represent high levels of stress, low levels of stress, acute stress and no stress, while three were designed for the entry-level to represent high levels of stress, low levels of stress and acute stress (\autoref{tab:stress-sample}). The predictions for all four participants can be found in \autoref{training-result}.

\begin{table}[htpb]
  \caption[Sample Stress Input]{Sample stress input for eating behavior prediction}\label{tab:stress-sample}
  \centering
  \begin{tabular}{l l l l l}
    \toprule
      & Stress Level & Average & Highest & Standard Deviation \\
    \midrule
      Day-Level High & & 4.0 & 5.0 & 0.816 \\
      Day-Level Low & & 1.667 & 2.0 & 0.471 \\
      Day-Level Acute & & 2.0 & 5.0 & 2.160 \\
      Day-Level No-Stress & & 0.0 & 0.0 & 0.0 \\
      Entry-Level High & 5.0 & 4.0 & 5.0 & 0.816 \\
      Entry-Level Low & 1.0 & 1.667 & 2.0 & 0.471 \\
      Entry-Level Acute & 5.0 & 2.0 & 5.0 & 2.160 \\
    \bottomrule
  \end{tabular}
\end{table}

Based on the predictions, text-based statements were made manually to describe eating behavior. A collection of such statements was sent back to the participants for evaluation in the form of a survey. A part of a survey containing a sample statement is shown below.

\begin{lstlisting}
Statement: It appears that you are likely to eat more than usual when
you're stressed.
Available responses:
* Oh my word! How did you know that!
* Yeah, sometimes
* Kind of...
* I don't often feel like that...
* Come on! I kept chatting with you for a month
  and look at your wrong judgement!
* Not sure
\end{lstlisting}

\bigskip
The responses to the statement, as listed above, were assigned scores 2, 1, 0, -1, -2, respectively, except for "Not sure", which whenever appears, the respective statement would not be counted into the evaluation. A complete collection of such statements used in this study is shown in \autoref{collect-statement}. The result of the user evaluation is discussed in the next chapter.
